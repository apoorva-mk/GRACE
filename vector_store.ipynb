{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load Documents and store within a ChromaDB vector DB following the MulitHop RAG example'''\n",
    "\n",
    "import chromadb\n",
    "import importlib\n",
    "JSONReader = importlib.import_module('submodules.MultiHop-RAG.util').JSONReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.storage import StorageContext\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.extractors import BaseExtractor\n",
    "from llama_index.ingestion import IngestionPipeline\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.llms import  OpenAI\n",
    "from llama_index import set_global_service_context, PromptHelper, ServiceContext, VectorStoreIndex\n",
    "from typing import List, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from corpus\n",
    "reader = JSONReader()\n",
    "data = reader.load_data('submodules/MultiHop_RAG/dataset/corpus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomExtractor(BaseExtractor):\n",
    "    async def aextract(self, nodes) -> List[Dict]:\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"title\": (\n",
    "                    node.metadata[\"title\"]\n",
    "                ),\n",
    "                \"source\": (\n",
    "                    node.metadata[\"source\"]\n",
    "                ),      \n",
    "                \"published_at\": (\n",
    "                    node.metadata[\"published_at\"]\n",
    "                )\n",
    "            }\n",
    "            for node in nodes\n",
    "        ]\n",
    "        return metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse inputs\n",
    "text_splitter = SentenceSplitter(chunk_size=256)\n",
    "\n",
    "transformations = [text_splitter,CustomExtractor()] \n",
    "pipeline = IngestionPipeline(transformations=transformations)\n",
    "nodes = await pipeline.arun(documents=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/home/zaristei/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 199.82it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 204.28it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 202.44it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 204.51it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 205.99it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 199.56it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 204.06it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 206.72it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 207.75it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 201.89it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 206.21it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 204.78it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 207.48it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 208.25it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 214.36it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 203.73it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 200.45it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:09<00:00, 206.01it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:10<00:00, 203.66it/s]\n",
      "Generating embeddings: 100%|██████████| 1549/1549 [00:07<00:00, 203.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Indexing...\n"
     ]
    }
   ],
   "source": [
    "# Create Index\n",
    "embed_model = HuggingFaceEmbedding(model_name='BAAI/llm-embedder', trust_remote_code=True)\n",
    "llm = OpenAI(model='gpt-3.5-turbo-1106', temperature=0, max_tokens=2048)\n",
    "prompt_helper = PromptHelper(\n",
    "    context_window=2048,\n",
    "    num_output=256,\n",
    "    chunk_overlap_ratio=0.1,\n",
    "    chunk_size_limit=None,\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    text_splitter=text_splitter,\n",
    "    prompt_helper=prompt_helper,\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "index = VectorStoreIndex(nodes, show_progress=True, storage_context=storage_context)\n",
    "print('Finish Indexing...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving from DB\n",
    "query_engine = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='77f88542-6f3b-4856-a18a-648565c86c80', embedding=None, metadata={'title': 'There’s something going on with AI startups in France', 'published_at': '2023-11-09T14:51:44+00:00', 'source': 'TechCrunch'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='135ad82d-3025-47e1-b10e-29ed3415bf7a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'There’s something going on with AI startups in France', 'published_at': '2023-11-09T14:51:44+00:00', 'source': 'TechCrunch'}, hash='254916260b14101b1935eb1ba88b838d7cecc57ed62f5ce8b480f7879beac393'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f4e88771-6568-4864-b5a3-a9e572057bbc', node_type=<ObjectType.TEXT: '1'>, metadata={'title': 'There’s something going on with AI startups in France', 'published_at': '2023-11-09T14:51:44+00:00', 'source': 'TechCrunch'}, hash='fe9e0da725559fa0db39ad42275b9cf782a297eb2bddba5ca12098f513bddf54'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8f492704-0f0b-4292-bf6a-b17fe322a7d0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='822b35a4fc7e05438bef84260474dbe0d001dc626686138baa9ed91ec8302275')}, text='Artificial intelligence, just like in the U.S., has quickly become a buzzy vertical within the French tech industry. But this time, France seems particularly well positioned to become one of the main AI hubs of the world.\\n\\nAnd it shouldn’t come as a surprise. Tech giants have historically set up AI research labs in Paris, as there’s a large talent pool of PhD students in math, computer science and engineering. They usually graduate from Polytechnique, ENS or Inria. They sometimes move to the U.S. for postdoctoral research and then move back to France to join a research lab.\\n\\nFor instance, Facebook (now Meta) created its Paris research lab back in 2015 with Yann LeCun at the helm of the AI initiative — this research group is called FAIR, for Facebook Artificial Intelligence Research. Google followed suit with its own AI research center dedicated to AI in Paris.\\n\\n“The FAIR team is extremely impressive. Just look at what they’ve done with Llama,” an entrepreneur told me yesterday.', start_char_idx=468, end_char_idx=1460, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6823604572601741),\n",
       " NodeWithScore(node=TextNode(id_='5fccfde0-20e0-43ff-9f9f-b24f7969f4f2', embedding=None, metadata={'title': 'There’s something going on with AI startups in France', 'published_at': '2023-11-09T14:51:44+00:00', 'source': 'TechCrunch'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='135ad82d-3025-47e1-b10e-29ed3415bf7a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'title': 'There’s something going on with AI startups in France', 'published_at': '2023-11-09T14:51:44+00:00', 'source': 'TechCrunch'}, hash='254916260b14101b1935eb1ba88b838d7cecc57ed62f5ce8b480f7879beac393'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='10432841-46e7-4619-8dc7-d28f2578dbff', node_type=<ObjectType.TEXT: '1'>, metadata={'title': \"Vikings vs. Broncos live score, updates, highlights from NFL 'Sunday Night Football' game\", 'published_at': '2023-11-19T23:00:04+00:00', 'source': 'Sporting News'}, hash='bb72a4dd74eee3f75a85731f2b7f91a7130bb0575323ac77f1da519656136e9f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f4e88771-6568-4864-b5a3-a9e572057bbc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='187bd109d8e7f7fed1d881b439acca39dc71236324df5a4d3897574837f755af')}, text='There’s something going on with AI startups in France In just a few months, dozens of French entrepreneurs have turned their focus to AI\\n\\nLast night, Motier Ventures held a tech meetup focused on AI startups featuring some of the most hyped tech startups in the French tech ecosystem: Dust, Finegrain, Gladia, Mistral AI and Scenario — all of them are portfolio companies of Motier Ventures. And you could feel a sense of excitement both onstage and in the audience.\\n\\nArtificial intelligence, just like in the U.S., has quickly become a buzzy vertical within the French tech industry. But this time, France seems particularly well positioned to become one of the main AI hubs of the world.\\n\\nAnd it shouldn’t come as a surprise. Tech giants have historically set up AI research labs in Paris, as there’s a large talent pool of PhD students in math, computer science and engineering. They usually graduate from Polytechnique, ENS or Inria. They sometimes move to the U.S.', start_char_idx=0, end_char_idx=969, text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6793820690507026)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "query_engine.retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement GraphRAG with NebulaGraph locally\n",
    "https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine.html#graph-rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
